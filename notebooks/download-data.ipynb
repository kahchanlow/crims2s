{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf8afa3-96b0-45f4-93e5-27a86e40928f",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "Download all data provided by the organizers.\n",
    "There are multiple data repositories, according to a nomenclature defined here https://github.com/ecmwf-lab/climetlab-s2s-ai-challenge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551c12a8-28bf-43c7-b8f1-afb7758ad839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import multiprocessing\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "INDEX_TRAIN_INPUT = 'https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-input/0.3.0/netcdf/index.html'\n",
    "INDEX_TRAIN_OUTPUT = 'https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-output-benchmark/index.html'\n",
    "INDEX_TRAIN_REFERENCE = 'https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-output-reference/index.html'\n",
    "\n",
    "INDEX_TEST_INPUT = 'https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/test-input/0.3.0/netcdf'\n",
    "INDEX_TEST_OUTPUT = 'https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/test-output-benchmark/index.html'\n",
    "INDEX_TEST_REFERENCE = 'https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/test-output-reference/index.html'\n",
    "HOME = \"C:\\\\Users\\\\klow55\\\\github\\\\crims2s\"\n",
    "TARGET_DIR = os.path.join(HOME,'hdd_scratch\\\\s2s')\n",
    "#os.makedirs(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02dd07c6-8a00-45be-b1ab-52762c48900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index(index_url):\n",
    "    html = requests.get(index_url).text\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    table = soup.find_all('tbody')[0]\n",
    "    links = table.find_all('a')\n",
    "    dataset_files = [a_tag.attrs['href'] for a_tag in links]\n",
    "    \n",
    "    return dataset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85be6db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>AccessDenied</Code><BucketName>s2s-ai-challenge</BucketName><RequestId>tx0000000000000152ef050-0062e48819-25c44be5-default</RequestId><HostId>25c44be5-default-default</HostId></Error>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_url=INDEX_TRAIN_REFERENCE\n",
    "html = requests.get(index_url).text\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30324369-f4e5-417c-b10a-006ac8c1adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_one_dataset_file(file_url):\n",
    "    \"\"\"Inspired by https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests.\"\"\"\n",
    "    url_path = urllib.parse.urlparse(file_url).path\n",
    "    download_path = pathlib.Path(TARGET_DIR) / pathlib.Path(url_path[23:])\n",
    "    download_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with requests.get(file_url, stream=True) as stream:\n",
    "        if os.path.isfile(download_path):\n",
    "            \"\"\"Ignore file if it already exists and file size is ok.\"\"\"\n",
    "            stream_len = int(stream.headers['Content-length'])\n",
    "            local_len = os.path.getsize(download_path)\n",
    "\n",
    "            if stream_len == local_len:               \n",
    "                return download_path\n",
    "        \n",
    "        \n",
    "        with open(download_path, 'wb') as f:\n",
    "            for chunk in stream.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                \n",
    "    return download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8df7474-52c0-4b40-9978-766bbfce2b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-output-benchmark/index.html\n",
      "https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-input/0.3.0/netcdf/index.html\n",
      "https://storage.ecmwf.europeanweather.cloud/s2s-ai-challenge/data/training-output-reference/index.html\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m [INDEX_TRAIN_OUTPUT, INDEX_TRAIN_INPUT, INDEX_TRAIN_REFERENCE, INDEX_TEST_REFERENCE, INDEX_TEST_INPUT, INDEX_TEST_OUTPUT]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[1;32m----> 4\u001b[0m     dataset_files\u001b[38;5;241m.\u001b[39mextend(\u001b[43mread_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mread_index\u001b[1;34m(index_url)\u001b[0m\n\u001b[0;32m      2\u001b[0m html \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(index_url)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      3\u001b[0m soup \u001b[38;5;241m=\u001b[39m bs4\u001b[38;5;241m.\u001b[39mBeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m links \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m dataset_files \u001b[38;5;241m=\u001b[39m [a_tag\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a_tag \u001b[38;5;129;01min\u001b[39;00m links]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataset_files = []\n",
    "for index in [INDEX_TRAIN_OUTPUT, INDEX_TRAIN_INPUT, INDEX_TRAIN_REFERENCE, INDEX_TEST_REFERENCE, INDEX_TEST_INPUT, INDEX_TEST_OUTPUT]:\n",
    "    print(index)\n",
    "    dataset_files.extend(read_index(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa02d3-8f0a-4026-8e5c-b0250b4dba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = download_one_dataset_file(dataset_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af71bc-c549-4ce0-9247-7d6542248f2d",
   "metadata": {},
   "source": [
    "## Test local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9059e1-6d82-4f98-8458-71fb21c0d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e88b6-11d4-419b-b48b-82080f846dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = xr.open_dataset(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f15ba6-c503-4b13-a090-9c8aca6c6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290bb0a-250f-4822-a780-d79174d7d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.lead_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad60401-eb58-4056-a224-2d1621fcd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.t2m.isel(lead_time=0, forecast_time=2, category=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115be162-8634-41a7-a751-4b9257097323",
   "metadata": {},
   "source": [
    "## Batch Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1161fa-d0b0-47f4-9006-d9da0c187e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.shuffle(dataset_files)\n",
    "\n",
    "with multiprocessing.Pool(processes=16) as pool:\n",
    "    for _ in tqdm(pool.imap(download_one_dataset_file, dataset_files), total=len(dataset_files)):\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1572b-5e26-4425-bd4b-96b0f79519f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4187826-8dce-4910-8b1d-a639ee79d1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
